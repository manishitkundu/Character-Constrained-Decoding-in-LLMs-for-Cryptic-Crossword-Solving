{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4096, out_features=128256, bias=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda:3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", token = '<token>').to(device)\n",
    "\n",
    "print(model.lm_head)\n",
    "\n",
    "# model.lm_head = Linear(\n",
    "#     in_features=model.lm_head.in_features,\n",
    "#     out_features=28,\n",
    "#     bias=False\n",
    "# ).to(device)\n",
    "\n",
    "# print(model.lm_head)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UCH', 'OK', 'ES', 'SB', 'ON', 'WE', 'ONS', 'USE', 'UC', 'CH', 'PS', 'SSL', 'QUE', 'BE', 'COD', 'OL', 'KER', 'ST', 'ATS', 'CK', 'SI', 'SO', 'O', 'QU', 'FT', 'SER', 'B', 'SES', 'OLS', 'uch', 'YS', 'OPS', 'US', 'AT', 'PE', 'SL', 'SION', 'OME', 'AK', 'ESS', 'ISS', 'CS', 'CE', 'UCE', 'IK', 'USB', 'ONY', 'SW', 'BS', 'UK']\n"
     ]
    }
   ],
   "source": [
    "# # Example input\n",
    "text = '''You are a cryptic crossword expert. You are given a clue for a cryptic crossword. Output only the answer. \n",
    "clue:\n",
    "Bend down to king in Chesterfield [6]\n",
    "output: CRO'''\n",
    "\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    \n",
    "logs = outputs.logits[0][-1]\n",
    "top_k = torch.topk(logs, 50)\n",
    "print([tokenizer.decode(top_k.indices[i]) for i in range(len(top_k.indices))])\n",
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Size: 16404\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "dir_path = \"/root/Cryptic-Crosswords/Crossword Clues/test\"\n",
    "test_sets = []\n",
    "for file in os.listdir(dir_path):\n",
    "    test_set_partial = pd.read_csv(os.path.join(dir_path,file))\n",
    "    test_sets.append(test_set_partial)\n",
    "\n",
    "# Combine all three DataFrames\n",
    "df = pd.read_csv(\"/root/Cryptic-Crosswords/Crossword Clues/test/unique_clues_test.csv\")\n",
    "\n",
    "clues = list(df[\"Clue\"])\n",
    "answers = list(df[\"Answer\"])\n",
    "lengths = list(df[\"Length\"])\n",
    "\n",
    "print(\"Test Set Size:\",len(clues))\n",
    "\n",
    "import json\n",
    "data = json.load(open('OtherTestSet/naive_random.json'))\n",
    "test_data = data['test']\n",
    "clues = [test_data[i]['clue'] for i in range(len(test_data))]\n",
    "answers = [test_data[i]['soln'] for i in range(len(test_data))]\n",
    "lengths = [test_data[i]['lengths'] for i in range(len(test_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Size: 28476\n",
      "Achy shaking stopped by iodine, salt and kaolin\n",
      "chinaclay\n",
      "[5, 4]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = json.load(open('OtherTestSet/naive_random.json'))\n",
    "test_data = data['test']\n",
    "clues = [test_data[i]['clue'] for i in range(len(test_data))]\n",
    "answers = [test_data[i]['soln'] for i in range(len(test_data))]\n",
    "lengths = [test_data[i]['lengths'] for i in range(len(test_data))]\n",
    "print(\"Test Set Size:\",len(clues))\n",
    "print(clues[0])\n",
    "print(answers[0])\n",
    "print(lengths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generator(clue,length, list_of_answer = [], repeat = 0):\n",
    "    prompt = f\"\"\"You are a cryptic crossword expert. You are given a clue for a cryptic crossword. Output only the answer. \n",
    "clue:\n",
    "{clue} {length}\n",
    "output:\"\"\"\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def beam_search_decoder(prompt, length_budget, model, tokenizer, initial_beam_width=50, beam_decay=0.7, max_steps=5, top_k=50):\n",
    "\n",
    "    if len(length_budget) == 1:\n",
    "        length_budget = length_budget[0]\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        BeamEntry = tuple[str, float]  # (generated_text, cumulative_logprob)\n",
    "        beam = [(\"\", 0.0)]\n",
    "        answer_list = []\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            current_beam_width = max(1, int(initial_beam_width * (beam_decay ** step)))\n",
    "            new_beam = []\n",
    "\n",
    "            for generated_text, cum_logprob in beam:\n",
    "                input_text = prompt + generated_text\n",
    "                input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_ids)\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                # Get top_k tokens from the last position\n",
    "                probs = torch.nn.functional.log_softmax(logits[0, -1], dim=-1)\n",
    "                topk_probs, topk_indices = torch.topk(probs, top_k)\n",
    "\n",
    "                for i in range(top_k):\n",
    "                    token_id = topk_indices[i].item()\n",
    "                    token_logprob = topk_probs[i].item()\n",
    "                    token_str = tokenizer.decode([token_id], skip_special_tokens=True)\n",
    "\n",
    "                    if not token_str.strip().isalpha():\n",
    "                        continue  # only alphanumeric tokens\n",
    "\n",
    "                    # Check if adding this token exceeds the length budget\n",
    "                    new_text = generated_text + token_str.strip()\n",
    "                    if len(new_text.strip()) > length_budget:\n",
    "                        continue\n",
    "                    elif len(new_text.strip()) == length_budget:\n",
    "                        answer_list.append((new_text, cum_logprob + token_logprob))\n",
    "                    else:\n",
    "                        new_beam.append((new_text, cum_logprob + token_logprob))\n",
    "\n",
    "            # Select top-k from new_beam\n",
    "            beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:current_beam_width]\n",
    "            # print(\"Step\", step, \"Beam:\", beam)\n",
    "            if not beam:\n",
    "                break\n",
    "\n",
    "        if answer_list:\n",
    "            final_answers = list(set([text.lower() for text, _ in sorted(answer_list, key=lambda x: x[1], reverse=True)]))\n",
    "            final_answers.sort()\n",
    "            return final_answers\n",
    "        elif beam:\n",
    "            return [beam[0][0]]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    else:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        BeamEntry = tuple[list[str], int, float]  # ([generated_words], current_word_idx, cumulative_logprob)\n",
    "        beam = [([], 0, 0.0)]\n",
    "        answer_list = []\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            current_beam_width = max(1, int(initial_beam_width * (beam_decay ** step)))\n",
    "            new_beam = []\n",
    "\n",
    "            for generated_words, word_idx, cum_logprob in beam:\n",
    "                if word_idx >= len(length_budget):\n",
    "                    continue  # Already done\n",
    "\n",
    "                current_word = \"\" if word_idx >= len(generated_words) else generated_words[word_idx]\n",
    "                input_text = prompt + \" \".join(generated_words + ([current_word] if current_word else []))\n",
    "                input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_ids)\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                probs = torch.nn.functional.log_softmax(logits[0, -1], dim=-1)\n",
    "                topk_probs, topk_indices = torch.topk(probs, top_k)\n",
    "\n",
    "                for i in range(top_k):\n",
    "                    token_id = topk_indices[i].item()\n",
    "                    token_logprob = topk_probs[i].item()\n",
    "                    token_str = tokenizer.decode([token_id], skip_special_tokens=True).strip()\n",
    "\n",
    "                    if not token_str.isalpha():\n",
    "                        continue\n",
    "\n",
    "                    new_word = current_word + token_str\n",
    "                    if len(new_word) > length_budget[word_idx]:\n",
    "                        continue\n",
    "\n",
    "                    new_generated_words = generated_words.copy()\n",
    "                    if len(new_word) == length_budget[word_idx]:\n",
    "                        # Completed a word\n",
    "                        if word_idx < len(generated_words):\n",
    "                            new_generated_words[word_idx] = new_word\n",
    "                        else:\n",
    "                            new_generated_words.append(new_word)\n",
    "\n",
    "                        if word_idx + 1 == len(length_budget):\n",
    "                            # Full sequence complete\n",
    "                            answer_list.append((\" \".join(new_generated_words), cum_logprob + token_logprob))\n",
    "                        else:\n",
    "                            new_beam.append((new_generated_words, word_idx + 1, cum_logprob + token_logprob))\n",
    "                    else:\n",
    "                        # Still building current word\n",
    "                        if word_idx < len(generated_words):\n",
    "                            new_generated_words[word_idx] = new_word\n",
    "                        else:\n",
    "                            new_generated_words.append(new_word)\n",
    "                        new_beam.append((new_generated_words, word_idx, cum_logprob + token_logprob))\n",
    "\n",
    "            beam = sorted(new_beam, key=lambda x: x[2], reverse=True)[:current_beam_width]\n",
    "            # print(\"Step\", step, \"Beam:\", beam)\n",
    "\n",
    "            if not beam:\n",
    "                break\n",
    "\n",
    "        if answer_list:\n",
    "            final_answers = list(set([text.lower() for text, _ in sorted(answer_list, key=lambda x: x[1], reverse=True)]))\n",
    "            final_answers.sort()\n",
    "            return final_answers\n",
    "        else:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:01<21:59,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 101/1000 [01:55<13:34,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 14.851485148514852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 201/1000 [03:53<15:11,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 12.437810945273633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 301/1000 [05:52<13:35,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 10.299003322259136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 401/1000 [07:47<11:27,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 10.972568578553616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 501/1000 [09:42<10:43,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 10.578842315369261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 601/1000 [11:38<08:19,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 10.8153078202995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 701/1000 [13:32<05:51,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 11.126961483594865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 801/1000 [15:23<03:48,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 10.986267166042447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 901/1000 [17:24<02:03,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 11.320754716981131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [19:19<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "acc = 0\n",
    "for idx in tqdm(range(len(clues[:1000]))):\n",
    "    # idx = 7\n",
    "    clue = clues[idx]\n",
    "    answer = answers[idx]\n",
    "    length = lengths[idx]\n",
    "    # print(answer)\n",
    "    # print(clue)\n",
    "    # print(length)\n",
    "    prompt = prompt_generator(clue,length)\n",
    "\n",
    "    decoded = beam_search_decoder(\n",
    "        prompt=prompt+\" \",\n",
    "        length_budget=length,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        initial_beam_width=20,\n",
    "        top_k=10,\n",
    "    )\n",
    "    acc += answer in decoded\n",
    "    # print(\"Generated:\", decoded)\n",
    "    # print(answer in decoded)\n",
    "    # print(len(decoded))\n",
    "\n",
    "    if idx%100==0:\n",
    "        print(idx, acc*100/(idx+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.305\n"
     ]
    }
   ],
   "source": [
    "print(acc/1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
